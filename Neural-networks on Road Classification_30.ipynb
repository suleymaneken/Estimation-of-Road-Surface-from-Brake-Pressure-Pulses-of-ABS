{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the needed module\n",
    "\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "import itertools\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from scipy.io import loadmat \n",
    "import pandas as pd\n",
    "plt.style.use('seaborn-poster')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#islak data\n",
    "w_aa_l = loadmat('./wet_30.mat')['sw_aw_30']\n",
    "w_aa_l = w_aa_l[:  , 1:]\n",
    "w_p_l = loadmat('./wet_30.mat')['sw_pp_30']\n",
    "w_p_l = w_p_l[:  , 1:]\n",
    "w_tv_l = loadmat('./wet_30.mat')['sw_tw_30']\n",
    "w_tv_l = w_tv_l[:  , 1:]\n",
    "w_vv_l = loadmat('./wet_30.mat')['sw_v_30']\n",
    "w_vv_l = w_vv_l[:  ,  1:]\n",
    "y = np.zeros((1646, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_aa_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.stack((w_aa_l, w_p_l,w_tv_l,w_vv_l,y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = loadmat('./slippery_30.mat')\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaygan data\n",
    "k_aa_l2 = loadmat('./slippery_30.mat')['sk_aw_30']\n",
    "k_aa_l2 = k_aa_l2[:  ,  1:] #iki tane kolon var\n",
    "p_l2 = loadmat('./slippery_30.mat')['sk_pp_30']\n",
    "p_l2 = p_l2[:  ,  1:]\n",
    "k_tv_l2 = loadmat('./slippery_30.mat')['sk_tw_30']\n",
    "k_tv_l2 = k_tv_l2[: , 1:]\n",
    "k_vv_l2 = loadmat('./slippery_30.mat')['sk_v_30']\n",
    "k_vv_l2 = k_vv_l2[:  ,  1:]\n",
    "y2 = np.ones((1445, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_aa_l2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kaygan=np.stack((k_aa_l2, p_l2,k_tv_l2,k_vv_l2,y2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toplam data\n",
    "new_30 = np.vstack((dataset, dataset_kaygan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = new_30.shape\n",
    "d2_train_dataset = new_30.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = r'datasets/purchase_data.xls'\n",
    "#purchase=pd.read_excel(file)\n",
    "#purchase.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "#Assign predictors to a variable of ndarray (matrix) type\n",
    "#array = purchase.values\n",
    "X = d2_train_dataset[:,0:3] # features\n",
    "y = d2_train_dataset[:,-1]\n",
    "\n",
    "#Divide records in training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples in training set: %d, number of samples in test set: %d'%(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ANN classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(40,40,40), activation='logistic', max_iter = 4000)\n",
    "\n",
    "# Train the classifier with the traning data\n",
    "mlp.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results from the test data\n",
    "predicted = mlp.predict(X_test_scaled)\n",
    "\n",
    "# plot the confusion matrix\n",
    "cm = confusion_matrix(y_test,predicted)\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A combination of under- and oversampling method using pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "over = SMOTE(sampling_strategy='minority')\n",
    "under = RandomUnderSampler(sampling_strategy='majority')\n",
    "steps = [('u', under), ('o', over), ('model', mlp)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# import libraries for evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy import mean\n",
    "\n",
    "# evaluate pipeline\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "score = mean(scores)\n",
    "print('ROC AUC score for the combined sampling method: %.3f' % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
